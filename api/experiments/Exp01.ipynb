{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (1.58.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (2.10)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: certifi in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mistralai in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (1.2.5)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from mistralai) (0.2.2)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from mistralai) (0.27.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from mistralai) (2.9.0.post0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from mistralai) (2.10.3)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from mistralai) (1.0.6)\n",
      "Requirement already satisfied: typing-inspect<0.10.0,>=0.9.0 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from mistralai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (1.3.1)\n",
      "Requirement already satisfied: anyio in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (4.7.0)\n",
      "Requirement already satisfied: idna in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (2.10)\n",
      "Requirement already satisfied: certifi in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->mistralai) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (4.12.2)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (2.27.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.8.2->mistralai) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from typing-inspect<0.10.0,>=0.9.0->mistralai) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/vihidun/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->mistralai) (1.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "openrouter_api_key = os.environ[\"OPENROUTER_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnLM_client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=openrouter_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "model = \"mistral-large-latest\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer_prompt(question, expected_answer, user_answer):\n",
    "    prompt = [{\"role\":\"system\", \"content\":f\"\"\"\n",
    "    You are an AI that checks answers for correctness.\n",
    "\n",
    "    - The question is: \"{question}\"\n",
    "    - The expected answer is: \"{expected_answer}\"\n",
    "    - The user's answer is: \"{user_answer}\"\n",
    "\n",
    "    Based on the user's answer compared to the expected answer, determine if the user's answer is **correct** or **incorrect**.\n",
    "\n",
    "    Output only in valid JSON format like this:\n",
    "\n",
    "    {{\n",
    "        \"result\": \"correct\"  # or \"incorrect\"\n",
    "    }}\n",
    "    \"\"\"}]\n",
    "\n",
    "    completion = learnLM_client.chat.completions.create(\n",
    "\n",
    "      model=\"google/learnlm-1.5-pro-experimental:free\",\n",
    "      messages=prompt\n",
    "    )\n",
    "\n",
    "    response = completion.choices[0].message.content\n",
    "    start_index = response.find('{')\n",
    "    end_index = response.rfind('}') + 1\n",
    "    json_text = response[start_index:end_index]\n",
    "\n",
    "    return json.loads(json_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def followup_que_gen(student_question):\n",
    "    messages = [{\"role\": \"system\",\n",
    "                 \"content\": f\"\"\"You are a Socratic tutor. This is the student question: '{student_question}'.\n",
    "                 Please generate **an appropriate number of follow-up questions**, each with an expected answer, to guide the student to the correct answer.\n",
    "                 Only generate the number of questions and answers that are necessary, up to a maximum of 10.\n",
    "                 If fewer questions are sufficient, stop at the appropriate point.\n",
    "\n",
    "                 Output the follow-up questions and their expected answers in **valid JSON format** like this:\n",
    "\n",
    "                 {{\n",
    "                     \"questions_and_answers\": [\n",
    "                         {{\"question\": \"What is the first step in solving the equation?\",\n",
    "                           \"answer\": \"Isolate the variable by subtracting 3 from both sides.\"}},\n",
    "                         {{\"question\": \"Can you identify the terms?\",\n",
    "                           \"answer\": \"The terms are 2x and 3 on the left side of the equation.\"}},\n",
    "                         ...\n",
    "                     ]\n",
    "                 }}\n",
    "\n",
    "                 Ensure the output is **valid JSON**, not truncated, and contains an appropriate number of questions and their corresponding expected answers, based on the complexity of the student's question.\"\"\"\n",
    "                }]\n",
    "\n",
    "    completion = learnLM_client.chat.completions.create(\n",
    "\n",
    "      model=\"google/learnlm-1.5-pro-experimental:free\",\n",
    "      messages=messages\n",
    "    )\n",
    "\n",
    "    response = completion.choices[0].message.content\n",
    "\n",
    "    start_index = response.find('{')\n",
    "    end_index = response.rfind('}') + 1\n",
    "    json_text = response[start_index:end_index]\n",
    "\n",
    "    return json.loads(json_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_question_index = 0 \n",
    "# questions_and_answers = []\n",
    "\n",
    "# def tutoring(student_answer, question, expected_answer,):\n",
    "    \n",
    "#     if current_question_index < len(questions_and_answers):\n",
    "#       int_response = check_answer_prompt(question, expected_answer, student_answer)\n",
    "#       is_correct = int_response.get(\"result\") == \"correct\"\n",
    "\n",
    "#       messages.append({\"role\": \"user\", \"content\":student_answer})\n",
    "#       if is_correct:\n",
    "#           print(questions_and_answers[current_question_index])\n",
    "#           messages.append({\"role\": \"tutor\", \"content\": questions_and_answers[current_question_index]})\n",
    "#           current_question_index +=1\n",
    "#       else:\n",
    "#           print(\"Try again!\")\n",
    "#           messages.append({\"role\": \"tutor\", \"content\": \"Try again!\"})\n",
    "\n",
    "\n",
    "#       completion = learnLM_client.chat.completions.create(\n",
    "\n",
    "#         model=\"google/learnlm-1.5-pro-experimental:free\",\n",
    "#         messages=messages\n",
    "#       )\n",
    "\n",
    "#       return completion.choices[0].message.content\n",
    "    \n",
    "#     else:\n",
    "#         print(\"over...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_question = \"It has been estimated that it will take 10 men 6 days to complete a certain task. Find the number of days it will take 8 men to complete a job which is double that task. Student Question: It has been estimated that it will take 10 men 6 days to complete a certain task. Find the number of days it will take 8 men to complete a job which is double that task.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': [{'question': 'If it takes 10 men 6 days to complete a task, how much work is done by one man in one day?', 'answer': 'Each man does 1/(10*6) = 1/60 of the task in one day.'}, {'question': 'Knowing that one man does 1/60 of the task per day, how much work can 8 men do in one day?', 'answer': '8 men can do 8 * (1/60) = 8/60 = 2/15 of the task in one day.'}, {'question': 'The problem states the new job is double the original task.  If the original task is represented by 1, how much work is the new job?', 'answer': 'The new job is double the original, so it represents 2 times the work, or 2.'}, {'question': 'If 8 men can complete 2/15 of the task in one day, and the new job is twice the size of the original task, how many days will it take them to complete the doubled task?', 'answer': 'It will take them 2 / (2/15) = 2 * (15/2) = 15 days.'}]}\n"
     ]
    }
   ],
   "source": [
    "response = followup_que_gen(student_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "socratic_tutor_description = \"Act as a socratic totor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "socratic_tutor_properties = \\\n",
    "{\n",
    "    # arg 1: section of memory to edit\n",
    "    \"student_question\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"Student question which is need to be act as a socratic tutor\",            \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core_tutoring_description = \"generate suitable response for the student answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core_tutoring_properties = \\\n",
    "# {\n",
    "#     \"student_answer\":{\n",
    "#         \"type\":\"string\",\n",
    "#         \"description\":\"Student answer which is need to give response\"\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool schema (passed to OpenAI)\n",
    "tool_metadata = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"socratic_tutor\",\n",
    "            \"description\": socratic_tutor_description,\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": socratic_tutor_properties,\n",
    "                \"required\": [\"student_question\"],\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    # {\n",
    "    #     \"type\": \"function\",\n",
    "    #     \"function\": {\n",
    "    #         \"name\": \"tutoring\",\n",
    "    #         \"description\": core_tutoring_description,\n",
    "    #         \"parameters\": {\n",
    "    #             \"type\": \"object\",\n",
    "    #             \"properties\": core_tutoring_properties,\n",
    "    #             \"required\": [\"student_answer\"],\n",
    "    #         },\n",
    "    #     }\n",
    "    # }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a Educational Tutor called LAAI. You must either call  \" \\\n",
    "    \"tools (socratic_tutor) or write response to the user. Do not take the same actions \" \\\n",
    "    \"multiple times! When student ask maths related question, make sure to always call the \" \\\n",
    "    \"socratic_tutor tool.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        # system prompt \n",
    "        {\"role\": \"system\", \"content\": system_prompt}, \n",
    "] \n",
    "\n",
    "current_question_index = 0 \n",
    "questions_and_answers = []\n",
    "\n",
    "def tutoring(student_answer, question, expected_answer,):\n",
    "    global current_question_index\n",
    "\n",
    "    if current_question_index < len(questions_and_answers):\n",
    "      int_response = check_answer_prompt(question, expected_answer, student_answer)\n",
    "      is_correct = int_response.get(\"result\") == \"correct\"\n",
    "\n",
    "      messages.append({\"role\": \"user\", \"content\":student_answer})\n",
    "      if is_correct:\n",
    "          print(questions_and_answers[current_question_index])\n",
    "          messages.append({\"role\": \"assistant\", \"content\": questions_and_answers[current_question_index]})\n",
    "          current_question_index +=1\n",
    "      else:\n",
    "          print(\"Try again!\")\n",
    "          messages.append({\"role\": \"assistant\", \"content\": \"Try again!\"})\n",
    "\n",
    "\n",
    "      completion = learnLM_client.chat.completions.create(\n",
    "\n",
    "        model=\"google/learnlm-1.5-pro-experimental:free\",\n",
    "        messages=messages\n",
    "      )\n",
    "\n",
    "      return completion.choices[0].message.content\n",
    "    \n",
    "    else:\n",
    "        print(\"over...\")\n",
    "        return(\"over...\")\n",
    "    \n",
    "\n",
    "# current_question_index = 0 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_step(user_message):\n",
    "    # user_message = input()\n",
    "    global current_question_index\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "\n",
    "    while True:\n",
    "        chat_completion = client.chat.complete(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tool_metadata\n",
    "        )\n",
    "        response = chat_completion.choices[0]\n",
    "\n",
    "        # update the messages with the agent's response \n",
    "        messages.append(response.message)\n",
    "\n",
    "\n",
    "        # if NOT calling a tool (responding to the user), return \n",
    "        if not response.message.tool_calls: \n",
    "            return response.message.content\n",
    "        \n",
    "        else: \n",
    "            print(\"TOOL CALL:\", response.message.tool_calls[0].function)\n",
    "            \n",
    "            # parse the arguments from the LLM function call\n",
    "            arguments = json.loads(\n",
    "                response.message.tool_calls[0].function.arguments\n",
    "            )\n",
    "\n",
    "            if response.message.tool_calls[0].function.name == \"followup_que_gen\":\n",
    "                \n",
    "                # run the function with the specified arguments\n",
    "                followup_que = followup_que_gen(**arguments)\n",
    "                print(followup_que)\n",
    "                \n",
    "                questions_and_answers = followup_que.get(\"questions_and_answers\", [])\n",
    "                if not questions_and_answers:\n",
    "                    raise ValueError(\"No follow-up questions generated\")\n",
    "                \n",
    "                # for item in questions_and_answers:\n",
    "                question = questions_and_answers[current_question_index].get(\"question\")\n",
    "                expected_answer = questions_and_answers[current_question_index].get(\"answer\")\n",
    "\n",
    "                if not question or not expected_answer:\n",
    "                    continue\n",
    "\n",
    "                print(question)\n",
    "                   \n",
    "                current_question_index += 1\n",
    "                messages.append({\"role\": \"assistant\", \"content\": question})\n",
    "            \n",
    "            if response.message.tool_calls[0].function.name == \"tutoring\":\n",
    "                tutoring_res = tutoring(**arguments)\n",
    "                print(tutoring_res)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            # add the tool call response to the message history \n",
    "            messages.append({\n",
    "                \"role\": \"tool\", \n",
    "                \"tool_call_id\": response.message.tool_calls[0].id, \n",
    "                \"name\": \"followup_que_gen\", \n",
    "                \"content\": f\"Updated memory: {response}\"\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm LAAI, your educational tutor. I'm here to help you with any questions or topics you're studying. How can I assist you today?\""
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_step(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm LAAI, your educational tutor. I'm here to help you with any questions or topics you're studying. How can I assist you today?\""
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_step(\"Who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apples can be various colors, including red, green, and yellow, depending on the variety.'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_step(\"what color is apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "SDKError",
     "evalue": "API error occurred: Status 400\n{\"object\":\"error\",\"message\":\"Unexpected role 'user' after role 'tool'\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSDKError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcan you help me to slove this question: It has been estimated that it will take 10 men 6 days to complete a certain task. Find the number of days it will take 8 men to complete a job which is double that task. Student Question: It has been estimated that it will take 10 men 6 days to complete a certain task. Find the number of days it will take 8 men to complete a job which is double that task.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[159], line 9\u001b[0m, in \u001b[0;36magent_step\u001b[0;34m(user_message)\u001b[0m\n\u001b[1;32m      5\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_message})\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     chat_completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_metadata\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     response \u001b[38;5;241m=\u001b[39m chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# update the messages with the agent's response \u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/mistralai/chat.py:145\u001b[0m, in \u001b[0;36mChat.complete\u001b[0;34m(self, model, messages, temperature, top_p, max_tokens, stream, stop, random_seed, response_format, tools, tool_choice, presence_penalty, frequency_penalty, n, safe_prompt, retries, server_url, timeout_ms)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mmatch_response(http_res, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4XX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5XX\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    144\u001b[0m     http_res_text \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mstream_to_text(http_res)\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m models\u001b[38;5;241m.\u001b[39mSDKError(\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI error occurred\u001b[39m\u001b[38;5;124m\"\u001b[39m, http_res\u001b[38;5;241m.\u001b[39mstatus_code, http_res_text, http_res\n\u001b[1;32m    147\u001b[0m     )\n\u001b[1;32m    149\u001b[0m content_type \u001b[38;5;241m=\u001b[39m http_res\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m http_res_text \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mstream_to_text(http_res)\n",
      "\u001b[0;31mSDKError\u001b[0m: API error occurred: Status 400\n{\"object\":\"error\",\"message\":\"Unexpected role 'user' after role 'tool'\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}"
     ]
    }
   ],
   "source": [
    "agent_step(\"can you help me to slove this question: It has been estimated that it will take 10 men 6 days to complete a certain task. Find the number of days it will take 8 men to complete a job which is double that task. Student Question: It has been estimated that it will take 10 men 6 days to complete a certain task. Find the number of days it will take 8 men to complete a job which is double that task.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
